{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "633e6100",
   "metadata": {},
   "source": [
    "## 1. Importing Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a13ebe5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/rahulshelke/Documents/Data-Science/Data-Science-Projects/bbc-news-sorting/notebooks'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# -- data loading --\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# -- visualization --\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# -- embeddings --\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# --- ml models ---\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "\n",
    "# -- model evaluation --\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "# -- utility --\n",
    "import mlflow\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c3e289",
   "metadata": {},
   "source": [
    "## 2. Loading Clean Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "16ef7b3f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 5)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df = pd.read_csv(\"data/train_clean_data.csv\")\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c7758f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ArticleId</th>\n",
       "      <th>Text</th>\n",
       "      <th>Category</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1833</td>\n",
       "      <td>worldcom ex-boss launches defence lawyers defe...</td>\n",
       "      <td>business</td>\n",
       "      <td>worldcom ex boss launches defence lawyers defe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>154</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>business</td>\n",
       "      <td>german business confidence slides german busin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1101</td>\n",
       "      <td>bbc poll indicates economic gloom citizens in ...</td>\n",
       "      <td>business</td>\n",
       "      <td>bbc poll indicates economic gloom citizens maj...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1976</td>\n",
       "      <td>lifestyle  governs mobile choice  faster  bett...</td>\n",
       "      <td>tech</td>\n",
       "      <td>lifestyle governs mobile choice faster better ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>917</td>\n",
       "      <td>enron bosses in $168m payout eighteen former e...</td>\n",
       "      <td>business</td>\n",
       "      <td>enron bosses payout eighteen former enron dire...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ArticleId                                               Text  Category  \\\n",
       "0       1833  worldcom ex-boss launches defence lawyers defe...  business   \n",
       "1        154  german business confidence slides german busin...  business   \n",
       "2       1101  bbc poll indicates economic gloom citizens in ...  business   \n",
       "3       1976  lifestyle  governs mobile choice  faster  bett...      tech   \n",
       "4        917  enron bosses in $168m payout eighteen former e...  business   \n",
       "\n",
       "                                          clean_text  Label  \n",
       "0  worldcom ex boss launches defence lawyers defe...      0  \n",
       "1  german business confidence slides german busin...      0  \n",
       "2  bbc poll indicates economic gloom citizens maj...      0  \n",
       "3  lifestyle governs mobile choice faster better ...      1  \n",
       "4  enron bosses payout eighteen former enron dire...      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eff907c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = train_df['Label'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02f555f",
   "metadata": {},
   "source": [
    "## 3. Cross Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "355197fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_k_fold_cv_f1(model, X, y, n_splits=5, shuffle=True, random_state=98):\n",
    "    \"\"\"\n",
    "    Perform Stratified K-Fold Cross Validation and return the average F1 score.\n",
    "    \n",
    "    Parameters:\n",
    "    - model: The machine learning model to evaluate.\n",
    "    - X: The feature matrix (data).\n",
    "    - y: The target vector (labels).\n",
    "    - n_splits: Number of splits/folds for cross-validation (default is 5).\n",
    "    - shuffle: Whether to shuffle the data before splitting (default is True).\n",
    "    - random_state: Seed for random number generator to ensure reproducibility.\n",
    "    \n",
    "    Returns:\n",
    "    - avg_f1: The average F1 score across all folds.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize StratifiedKFold with specified number of splits and options\n",
    "    skf = StratifiedKFold(n_splits=n_splits, shuffle=shuffle, random_state=random_state)\n",
    "    \n",
    "    f1_scores = []  # List to store F1 score for each fold\n",
    "    \n",
    "    # Split the data into train and test sets for each fold\n",
    "    for train_index, test_index in skf.split(X, y):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        \n",
    "        # Train the model on the training data\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions on the test data\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate and store the F1 score for this fold\n",
    "        f1_scores.append(f1_score(y_test, y_pred, average='weighted'))\n",
    "    \n",
    "    # Calculate the average F1 score across all folds\n",
    "    avg_f1 = sum(f1_scores) / len(f1_scores)\n",
    "    avg_std_f1 = np.std(f1_scores)\n",
    "    return avg_f1, avg_std_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c221968",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_performance(model, X, y, n_splits=5, shuffle=True, random_state=98):\n",
    "    avg_f1, std_f1 = stratified_k_fold_cv_f1(model, X, y, n_splits, shuffle, random_state)\n",
    "    print(f\"Train Avg F1 Score: {avg_f1} Std: {std_f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d8e513",
   "metadata": {},
   "source": [
    "## 4. One-Hot Encoding\n",
    "\n",
    "**Steps:**\n",
    "1. **Create a vocabulary:** A vocabulary is a list of unique words from your corpus (the entire set of documents).\n",
    "\n",
    "2. **Generate one-hot vectors:** For each word in the corpus, create a vector of zeros where the index corresponding to that word in the vocabulary is set to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88986072",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = train_df[\"clean_text\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9e599f2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Create a vocabulary (a set of unique words)\n",
    "def create_vocabulary(corpus):\n",
    "    vocabulary = set()\n",
    "    for text in corpus:\n",
    "        words = text.split()  # Convert to lower case and split by spaces\n",
    "        vocabulary.update(words)\n",
    "    return sorted(vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "07ce2e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the vocabulary\n",
    "vocabulary = create_vocabulary(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4192705a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: ['applications', 'applied', 'applies', 'apply', 'applying', 'appoint', 'appointed', 'appointment', 'appointments', 'appoints']\n"
     ]
    }
   ],
   "source": [
    "print(\"Vocabulary:\", vocabulary[1000:1010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7115937d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: One-hot encoding function\n",
    "def one_hot_encode(corpus, vocabulary):\n",
    "    # Create a mapping of words to their respective indices in the vocabulary\n",
    "    word_to_index = {word: index for index, word in enumerate(vocabulary)}\n",
    "    \n",
    "    # Initialize a list to hold the one-hot encoded vectors for each document\n",
    "    one_hot_encoded_corpus = []\n",
    "    \n",
    "    for text in corpus:\n",
    "        # Initialize a vector of zeros of the same length as the vocabulary\n",
    "        one_hot_vector = np.zeros(len(vocabulary))\n",
    "        \n",
    "        # For each word in the document, set the corresponding position in the vector to 1\n",
    "        for word in text.lower().split():\n",
    "            index = word_to_index[word]  # Get the index of the word in the vocabulary\n",
    "            one_hot_vector[index] = 1   # Set the corresponding index to 1\n",
    "        \n",
    "        one_hot_encoded_corpus.append(one_hot_vector)\n",
    "    \n",
    "    return np.array(one_hot_encoded_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97fa504d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the one-hot encoded corpus\n",
    "one_hot_corpus = one_hot_encode(corpus, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d8383727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 23408)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_hot_corpus.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "de0638ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.DataFrame(one_hot_corpus, columns=vocabulary)\n",
    "\n",
    "# # Plot the heatmap using seaborn\n",
    "# plt.figure(figsize=(10, 6))\n",
    "# sns.heatmap(df, annot=True, cmap=\"YlGnBu\", cbar=True, xticklabels=df.columns, yticklabels=[f\"Doc {i+1}\" for i in range(df.shape[0])])\n",
    "\n",
    "# # Customize the title and labels\n",
    "# plt.title(\"OneHot Heatmap\", fontsize=16)\n",
    "# plt.xlabel(\"Terms\", fontsize=12)\n",
    "# plt.ylabel(\"Documents\", fontsize=12)\n",
    "\n",
    "# # Display the heatmap\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd96a6a0",
   "metadata": {},
   "source": [
    "### 4.1 Logistic Regression on One Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "86c88bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %time\n",
    "# # Start an experiment (if not already created)\n",
    "# experiment_name = \"One_Hot_Embeddings\"\n",
    "# mlflow.set_experiment(experiment_name)\n",
    "\n",
    "# model_name = \"Logistic Regression\"\n",
    "# model_save_name = \"logistic_regression_model\"\n",
    "\n",
    "# # Start MLflow run\n",
    "# with mlflow.start_run(run_name=f\"{experiment_name} + {model_name}\"):\n",
    "\n",
    "#     # initializing model\n",
    "#     classifier = LogisticRegression(max_iter=2000)\n",
    "\n",
    "#     # cross validation\n",
    "#     avg_f1_score, avg_f1_std = stratified_k_fold_cv_f1(classifier, one_hot_encode, y)\n",
    "\n",
    "#     # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "#     mlflow.log_metric(\"avg f1 score\", avg_f1_score)\n",
    "#     mlflow.log_metric(\"avg f1 std\", avg_f1_std)\n",
    "#     mlflow.log_param(\"models\", model_name)\n",
    "\n",
    "#     print(f\"Logged {model_name} model with avg f1 score: {avg_f1_score}\")\n",
    "#     print(f\"Logged {model_name} model with avg f1 std: {avg_f1_std}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "84806f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea7c4cc",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d356d684",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 5185.47it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Logistic Regression model with training f1 score: 1.0\n",
      "Logged Logistic Regression model with testing f1 score: 0.9642058165548099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start an experiment (if not already created)\n",
    "mlflow.set_experiment(\"One_Hot_Embeddings\")\n",
    "\n",
    "model_name = \"Logistic Regression\"\n",
    "model_save_name = \"logistic_regression_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"One Hot Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c244d227",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 120.66it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Support Vector Classifier model with training f1 score: 1.0\n",
      "Logged Support Vector Classifier model with testing f1 score: 0.9485458612975392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Support Vector Classifier\"\n",
    "model_save_name = \"support_vector_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"One Hot Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = SVC()\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "1c14e5f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 4122.46it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Random Forest Classifier model with training f1 score: 1.0\n",
      "Logged Random Forest Classifier model with testing f1 score: 0.9552572706935123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Random Forest Classifier\"\n",
    "model_save_name = \"random_forest_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"One Hot Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = RandomForestClassifier()\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "7b2d3138",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 5495.06it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Gradient Boosting Classifier model with training f1 score: 1.0\n",
      "Logged Gradient Boosting Classifier model with testing f1 score: 0.9485458612975392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Gradient Boosting Classifier\"\n",
    "model_save_name = \"gradient_boosting_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"One Hot Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = GradientBoostingClassifier(verbose=False)\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b1ab53",
   "metadata": {},
   "source": [
    "## 5. TF-IDF Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "77e84a0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "tfidf_matrix = np.asarray(tfidf_matrix.toarray())\n",
    "\n",
    "# Get the feature names (i.e., terms in the vocabulary)\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "\n",
    "# Convert the TF-IDF matrix to a dense format (for better visualization)\n",
    "dense_matrix = tfidf_matrix#.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4699c01a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1490, 23390)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dense_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bfc6d641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1043, 23390), (1043,), (447, 23390), (447,))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dense_matrix, y, test_size=0.3, stratify=y, random_state=42)\n",
    "\n",
    "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd53e874",
   "metadata": {},
   "source": [
    "### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6fe17370",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 4576.08it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Logistic Regression model with training f1 score: 0.9990412272291467\n",
      "Logged Logistic Regression model with testing f1 score: 0.9530201342281879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Start an experiment (if not already created)\n",
    "mlflow.set_experiment(\"TF_IDF_Embeddings\")\n",
    "\n",
    "model_name = \"Logistic Regression\"\n",
    "model_save_name = \"logistic_regression_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"TF-IDF Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = LogisticRegression(max_iter=1000)\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "34fdec62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 121.76it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Support Vector Classifier model with training f1 score: 1.0\n",
      "Logged Support Vector Classifier model with testing f1 score: 0.9530201342281879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Support Vector Classifier\"\n",
    "model_save_name = \"support_vector_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"TF-IDF Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = SVC()\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6d0c6937",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 4614.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Random Forest Classifier model with training f1 score: 1.0\n",
      "Logged Random Forest Classifier model with testing f1 score: 0.9530201342281879\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Random Forest Classifier\"\n",
    "model_save_name = \"random_forest_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"TF-IDF Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = RandomForestClassifier()\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "8bf688e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading artifacts: 100%|██████████| 7/7 [00:00<00:00, 5352.80it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logged Gradient Boosting Classifier model with training f1 score: 1.0\n",
      "Logged Gradient Boosting Classifier model with testing f1 score: 0.9507829977628636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Gradient Boosting Classifier\"\n",
    "model_save_name = \"gradient_boosting_model\"\n",
    "\n",
    "# Start MLflow run\n",
    "with mlflow.start_run(run_name=f\"TF-IDF Enbedding + {model_name}\"):\n",
    "\n",
    "    # initializing model\n",
    "    classifier = GradientBoostingClassifier(verbose=False)\n",
    "    # Train the model\n",
    "    classifier.fit(X_train, y_train)\n",
    "\n",
    "    # Predict\n",
    "    y_pred_train = classifier.predict(X_train)\n",
    "    y_pred_test = classifier.predict(X_test)\n",
    "\n",
    "    # Evaluate the model\n",
    "    f1_score_train = f1_score(y_train, y_pred_train, average='micro')\n",
    "    f1_score_test = f1_score(y_test, y_pred_test, average='micro')\n",
    "    train_report = classification_report(y_train, y_pred_train, output_dict=True)\n",
    "    test_report = classification_report(y_test, y_pred_test, output_dict=True)\n",
    "\n",
    "    # Log parameters (e.g., model hyperparameters)\n",
    "    mlflow.log_param(\"model\", f\"{model_name}\")\n",
    "\n",
    "    # Log metrics (e.g., f1 score, precision, recall, F1-score)\n",
    "    mlflow.log_metric(\"train f1 score\", f1_score_train)\n",
    "    for label, metrics in train_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    mlflow.log_metric(\"test f1 score\", f1_score_test)\n",
    "    for label, metrics in test_report.items():\n",
    "        if isinstance(metrics, dict):  # Only log metrics that are numeric (e.g., per-class metrics)\n",
    "            for metric_name, metric_value in metrics.items():\n",
    "                mlflow.log_metric(f\"{label}_{metric_name}\", metric_value)\n",
    "\n",
    "    # Log the trained model\n",
    "    mlflow.sklearn.log_model(classifier, model_save_name, input_example=X_train[0].reshape(1, -1))\n",
    "\n",
    "    # End the MLflow run (automatically done when exiting the context)\n",
    "    # print(f\"Model trained and logged with run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "    print(f\"Logged {model_name} model with training f1 score: {f1_score_train}\")\n",
    "    print(f\"Logged {model_name} model with testing f1 score: {f1_score_test}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
